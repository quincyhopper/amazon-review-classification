# Amazon review classifier
This project implements a moduler feed forward network (FNN) from scratch using NumPy to classify reviews. It includes a custom preprocessing pipeline, bag-of-words and pre-trained GloVe embedding vectorisation, TF-IDF token weighting, and an automated training engine with learning rate scheduling. 

# Requirements
- NumPy 2.3.5
- tqdm 4.67.1 
- Pandas 2.3.3 for saving data to csv (training logs, classification reports, etc)

# Installation
1. Clone the repository
```
git clone https:
```
2. Install dependencies 
```
pip install -r requirements.txt
```

# Review data
Make sure `Compiled_Reviews.txt` is in the `data/` directory, then run the following command to convert the data to CSV format:
```
python to_csv.py
```

# GloVe embeddings
To use the GloveVectoriser:
1. Download the 100d or 200d dimensions from https://nlp.stanford.edu/projects/glove/
2. Place the .txt files in the `data/` directory.

# Project structure
```
.
├── data/                    
│   └── Compiled_Reviews.txt
│   └── reviews.csv         # Generated by to_csv.py
│   └── 200.glove.txt       # GloVe embeddings (ignored by git)
├── config.py               # Hyperparameters for splitting, preprocessing and training
├── encoder.py              # Bag-of-word, GloVe and TFI-IDF encoders
├── engine.py               # Training and evaluation loops
├── losses.py               # Cross entropy loss function
├── main.py                 # Main execution script
├── metrics.py              # Confusion matrix and classification reports
├── nn.py                   # Neural Network class
├── optim.py                # Optimisation algorithms and learning rate scheduler
├── preprocessing.py        # Tokenisation and text preprocessing
├── README.md               # Project documentation
├── requirements.txt        # Requirements
├── to_csv.py               # Convert raw review data to csv
├── utils.py                # Data loading, splitting and helper functions
└── .gitignore              # Files to exclude from GitHub
```

# How to run
Once the data is prepared in the `data/` folder, run the following command to train a model:
```
python main.py
```
This command will:
1. Load the data
2. Tokenise, split and clean the data
3. Vectorise the reviews
4. Train a feed forward network
5. Evaluate the model after training is finished

# Configuration
All preprocessing settings and model hyperparameters are stored in `config.py`. You can modify the following without touching the main logic:
1. **Random state**: The random number generator used in sampling and parameter initialisation (e.g `42`).
2. **Data splits**: Change the size of the data splits (e.g. `'val_size': 0.2`)
3. **Preprocessing**: Toggle `lowercase`, `remove_punct`, `count_threshold` and more. 
4. **Vectorisation**: Switch between `bow` and `glove` and optionally use `'weighting': 'tfidf'`
5. **Model architecture**: Change the `hidden_layers` list (e.g. `[512, 256]`)
6. **Training hyperparameters**: Change `learning_rate`, `optimiser`, `batch_size`, etc. 

# Technical highlights 
## Backpropagation
The `FeedForwardNetwork` class in `nn.py` can implement backpropagation for an arbitrary number of layers, each containing an arbitrary number of neurons/units. The class uses the chain rule to compute the gradient of the output with respect to every layer in the network (Linear or ReLU). This allows you to test a variety of architectures without having to rewrite the logic. **Note that ReLU activation is hard-coded**.

## AdamW optimisation
The `Adam` class in `optim.py` allows the model to update its weights and biases using the AdamW optimisation algorithm. It includes decoupled weight decay.

## Term-frequency inverse document frequency (TF-IDF)
The `TFIDF` class in `encoder.py` allows you to optionally apply TF-IDF weighting to both BoW and GloVe vectors.

# References
- Loshchilov, I. and Hutter, F. (2019) Decoupled weight decay regularization, arXiv.org. Available at: https://arxiv.org/abs/1711.05101.
- Carlson, R., Bauer, J. and Manning, C.D. (2025) A new pair of gloves, arXiv.org. Available at: https://arxiv.org/abs/2507.18103. 